FABRICE COLLARD
HARRIS DELLAS

The Great Inflation of the 1970s
The two leading explanations for the poor inflation performance during the
1970s are policy opportunism (Barro and Gordon 1983) and “inadvertently”
bad monetary policy (Clarida, Gali, and Gertler 2000, Orphanides 2003). The
main models of the latter category are that of Orphanides (loose monetary
policy was the outcome of mis-perceptions about potential output rather than
of inflation tolerance) and of Clarida, Gali, and Gertler (weak policy reaction
to expected inflation led to indeterminacies). We show that both of these
models can account for high and persistent inflation and also have satisfactory
overall performance if an exceptionally large decrease in productivity took
place at that time.
JEL codes: E32, E52
Keywords: inflation, imperfect information, learning, monetary policy
rule, indeterminacy.

DURING THE 1970s, the inflation rate in the United States
reached its 20th century peak, with levels exceeding 10%. The causes of this “great”
inflation remain the subject of considerable academic debate. Broadly speaking, the
proposed explanations fall into two categories. Those that claim that the high inflation
was due to the lack of proper incentives on the part of policy makers who chose to
accept (or even induce) high inflation in order to prevent a recession (the inflation
bias suggested by Barro and Gordon 1983; see also Ireland 1999). And those that
claim that it may have been the result of the honest mistakes of a well-meaning central bank. The latter category can be further subdivided into a group of explanations
that emphasize either bad luck under significant imperfect information or bad luck
together with technical, inadvertent errors in policy design.

We would like to thank an anonymous referee as well as Alex Cukierman, Bob King, Andy Levin, Frank
Schforheide, Mike Spagat, Mike Woodford, and the participants at the International Research Forum on
Monetary Policy in Washington, D.C. and at the European Monetary Forum in Bonn for valuable comments.
Dellas is grateful to Ecoscientia Stiftung for generous financial support.

FABRICE COLLARD is at CNRS-GREMAQ, Manufacture des Tabacs, bât. F, 21 allée de Brienne,
31000 Toulouse, France (E-mail: fabrice.collard@gremaq.univ-tlse1.fr). HARRIS DELLAS is at
the Department of Economics, University of Bern, CEPR (E-mail: harris.dellas@vwi.unibe.ch,
Homepage: http://www.vwi.unibe.ch/amakro/dellas.htm).
Received November 2, 2005; and accepted in revised form January 20, 2006.
Journal of Money, Credit and Banking, Vol. 39, No. 2–3 (March–April 2007)

C 2007 The Ohio State University

714

:

MONEY, CREDIT AND BANKING

According to the latter view, the U.S. Federal Reserve Bank (FED) inadvertently
committed a “technical” error by implementing an interest policy rule in which nominal interest rates were moved less than expected inflation (Clarida, Gali, and Gertler
2000). The resulting decrease in real interest rates fueled inflation inducing instability
(indeterminacy) in the economy and exaggerating inflation movements. The implication of this view is that adoption of the standard Henderson–McKibbin–Taylor (HMT)
rule would have prevented the persistent surge in inflation.
The bad luck under imperfect information view claims that loose monetary policy
and inflation reflected an unavoidable mistake on the part of a monetary authority
whose tolerance of inflation did not differ significantly from that commonly attributed
to the authorities in the 1980s and 1990s. Orphanides (2003, 2004) has argued that the
large decrease in actual output following the persistent downward shift in potential
output was interpreted as a decrease in the output gap. 1 It led to expansionary monetary
policy that exaggerated the inflationary impact of the decrease in potential output.
Eventually and after a long delay, the FED realized that potential output growth was
lower and adjusted policy to bring inflation down. Imperfect information about the
substantial productivity slowdown rather than tolerance of inflation played the critical
role in the inflation process.
All these theories seem plausible. Identifying the most empirically relevant one has
not been an easy task. A subset of the literature has tackled the issue of the contribution
of policy to inflation directly, by examining whether monetary actions can be captured
by a policy rule, and if yes, what the properties of such rule are. Relying on single
equation estimation, Clarida, Gali, and Gertler (2000) claim that the FED indeed
followed an interest rule during the 1970s but that rule contained a weak reaction to
inflation that led to indeterminacies. Orphanides (2004) disputes this claim. Using
real-time data, he documents the existence of a rule too, but he also finds no significant
difference between pre- and post-Volcker tolerance regarding inflation. Lubik and
Schforheide (2003) estimate a small new Keynesian model (without learning, though,
on the part of monetary authorities) and arrive at results similar to those of Clarida,
Gertler, and Gali’s (see also Canova 2005). According to their estimated model, post1982 U.S. monetary policy is consistent with determinacy, whereas the pre-Volcker
policy is not. Nelson and Nicolov (2002) estimate a similar small scale model for
the U.K. and find that both output gap mismeasurement and a weak policy response
to inflation played an important role. And that the weak reaction to inflation does
not seem to have encouraged multiple equilibria. Finally, Orphanides and Williams
(2005) estimate a small-scale model of the U.S. economy and argue that the source
of the high inflation is to be found in the aggressive response to the unemployment
gap.
1. Related explanations are that the FED was the “victim” of conventional macroeconomic wisdom of
the time that claimed the existence of a stable, permanent trade-off between inflation and unemployment
(De Long 1997), or, that the FED was the “victim” of econometrics. Sargent (1999) for instance, has argued
that the data periodically give the impression of the existence of a Phillips curve with a favorable trade-off
between inflation and unemployment. High inflation then results as the central bank attempts to exploit
this.

FABRICE COLLARD AND HARRIS DELLAS

:

715

A second subset of the literature again uses a small-scale model but imposes—
rather than estimates—a policy rule. Lansing (2001) finds that a specification with
sufficiently large reaction to inflation is consistent with the patterns of inflation and
output observed during the 1970s.
Finally, a third subset of the empirical literature has investigated the events of
the 1970s within the context of calibrated, stochastic general equilibrium models.
Christiano and Gust (1999) argue that the new Keynesian model cannot replicate
that experience, while a limited participation model with indeterminacy can (they
do not address the role of imperfect information, though). Cukierman and Lippi
(2005) demonstrate how, within a backward looking version of the Keynesian model,
imperfect information leads to serially correlated forecast errors and loose monetary
policy. Bullard and Eusepi (2003) argue that a persistent increase in inflation can
obtain in the new Keynesian model even when policy responds strongly to inflation
when the policy makers learn gradually about changes in trend productivity. Finally,
in related work which, however, looks at the disinflation of the 1980s, Erceg and
Levin (2003) argue that the disinflation experience can be accounted for by a shift
in the inflation target of the FED with the public only gradually learning about the
policy regime switch.
Our objective in this paper is twofold. First, to examine whether explanations based
on rules—as opposed to discretion—are consistent with the macroeconomic performance of the 1970s. We emphasize overall macroeconomic performance because we
find attempts to validate particular theories based solely on the behavior of inflation
too narrow. And second, to undertake a direct comparison of the two leading explanations from this group (Orphanides vs. Clarida, Gali, and Gertler). This is an important
task, as the two explanations carry dramatically different implications for inflation
scenarios in the future. If the Orphanides view is correct, then strong reaction to expected inflation is not sufficient to prevent bad inflation outcomes. The experience of
the 1970s can be repeated. If the Clarida, Gali, and Gertler view is correct, then inflation is likely to remain tamed as long as the central bank reacts sufficiently strongly
to expected inflation.
We address these questions within the new Keynesian (NK) model. We ask whether
and under what conditions the NK model with policy commitment can replicate the
evolution of inflation following a severe, persistent slowdown in the rate of productivity growth. And if yes, whether the model also meets additional fitness criteria.
We first examine whether the model can generate a “great inflation” under the
assumption that the HMT policy rule pursued at the time did not differ from that
commonly attributed to the “Volcker–Greenspan” FED (see Clarida, Gali, and Gertler
2000, Orphanides 2004). We find that this is the case if the productivity slowdown
is very large and there exists a high degree of imperfect information. 2 Imperfect
information introduces stickiness in inflation forecasts, making the expected inflation
“gap”(the deviation of expected from target inflation) small. The underestimation
2. We follow Svensson and Woodford (2003) in modeling imperfect information using the Kalman
filter.

716

:

MONEY, CREDIT AND BANKING

of the inflation gap leads to weak policy reaction even when the inflation reaction
coefficient is large. We also find that the overall macroeconomic performance of this
model is good with two exceptions: The predicted recession is too severe. And the
required shock is extraordinarily large.
We then examine the performance of the model under HMT rules that allow for indeterminacy (following Clarida, Gali, and Gertler) due to a small reaction coefficient
to inflation. Some of these rules have good properties: They generate inflation persistence and realistic overall macroeconomic volatility. Their main weakness, though,
is that they too require a large shock (but nonetheless much smaller than in the case
under imperfect information) and also generate too severe of a recession.
The rest of the paper is organized as follows. Section 1 presents the model. Section 2
discusses the calibration. Section 3 presents the main results. An Appendix describes
the mechanics of the solution to the model under imperfect information and learning
based on the Kalman filter.

1. THE MODEL
The set up is the standard NK model. The economy is populated by a large number
of identical infinitely lived households and consists of two sectors: one producing
intermediate goods and the other a final good. The intermediate good is produced
with capital and labor and the final good with intermediate goods. The final good is
homogeneous and can be used for consumption (private and public) and investment
purposes.
1.1 The Household
Household preferences are characterized by the lifetime utility function: 3


∞

Mt+τ
τ
E t β U Ct+τ ,
, t+τ ,
Pt+τ
τ =0

(1)

where 0 < β < 1 is a constant discount factor, Ct denotes the domestic consumption
bundle, Mt /Pt is real balances and  t is the quantity of leisure enjoyed by the representative household. The utility function, U (Ct , Mt /Pt , t ) : R+ × R+ × [0, 1] −→ R
is increasing and concave in its arguments.
The household is subject to the following time constraint
t + h t = 1,

(2)

where h denotes hours worked. The total time endowment is normalized to unity.
3. E t (·) denotes mathematical conditional expectations. Expectations are conditional on information
available at the beginning of period t.

FABRICE COLLARD AND HARRIS DELLAS

:

717

In each and every period, the representative household faces a budget constraint of
the form
Bt+1 + Mt + Pt (Ct + It + Tt )  Rt−1 Bt + Mt−1 + Nt + t
+ Pt Wt h t + Pt z t K t ,

(3)

where Wt is the real wage; Pt is the nominal price of the final good; Ct is consumption
and It is investment expenditure; and K t is the amount of physical capital owned by
the household and leased to the firms at the real rental rate z t . M t−1 is the amount of
money that the household brings into period t, and Mt is the end of period t money
holdings. Nt is a nominal lump-sum transfer received from the monetary authority;
Tt is the lump-sum taxes paid to the government and used to finance government
consumption.
Capital accumulates according to the law of motion
2

ϕ It
K t+1 = It −
− δ K t + (1 − δ)K t ,
(4)
2 Kt
where δ ∈ [0, 1] denotes the rate of depreciation. The second term captures the
existence of capital adjustment costs. ϕ > 0 is the capital adjustment costs parameter.
The household determines her consumption/savings, money holdings, and leisure
plans by maximizing her utility (1) subject to the time constraint (2), the budget
constraint (3), and taking the evolution of physical capital (4) into account.
1.2 Final Goods Sector
The final good is produced by combining intermediate goods. This process is
described by the following CES function

Yt =

1

 θ1

θ

X t (i) di

,

(5)

0

where θ ∈ (−∞, 1). θ determines the elasticity of substitution between the various
inputs. The producers in this sector are assumed to behave competitively and to
determine their demand for each good, X t (i), i ∈ (0, 1) by maximizing the static
profit equation
 1
max Pt Yt −
Pt (i)X t (i) di,
(6)
{X t (i)}i∈(0,1)

0

subject to (5), where Pt (i) denotes the price of intermediate good i. This yields demand
functions of the form:
 1

Pt (i) θ−1
X t (i) =
Yt for i ∈ (0, 1)
(7)
Pt
and the following general price index

718

:

MONEY, CREDIT AND BANKING



1

Pt =

Pt (i)

θ
θ−1

 θ −1
θ
di

.

(8)

0

The final good may be used for consumption—private or public—and investment
purposes.
1.3 Intermediate Goods Producers
Each firm i, i ∈ (0, 1), produces an intermediate good by means of capital and labor
according to a constant returns-to-scale technology, represented by the Cobb–Douglas
production function
X t (i) = At K t (i)α h t (i)1−α

with α ∈ (0, 1),

(9)

where K t (i) and h t (i), respectively, denote the physical capital and the labor input
used by firm i in the production process. At is an exogenous stationary stochastic
technology shock, whose properties will be defined later. Assuming that each firm
i operates under perfect competition in the input markets, the firm determines its
production plan so as to minimize its total cost
min

{K t (i),h t (i)}

Pt Wt h t (i) + Pt z t K t (i)

subject to (9). Total costs then take the form Pt St Xt (i) where the real marginal cost,
z αt /χ At with χ = α α (1 − α)1−α .
St , is given by W 1−α
t
Intermediate goods producers are monopolistically competitive, and therefore set
prices for the good they produce. We follow Calvo in assuming that firms set their
prices for a stochastic number of periods. In each and every period, a firm either gets
the chance to adjust its price (an event occurring with probability γ ) or it does not. In
order to maintain long-term money neutrality (in the absence of monetary frictions)
we also assume that the price set by the firm grows at the steady-state rate of inflation.
Hence, if a firm i does not reset its price, the latter is given by Pt (i) = π̄ Pt−1 (i). A
firm i sets its price, p̃t (i), in period t in order to maximize its discounted profit flow:
˜ t (i) + E t
max 
p̃t (i)

∞


˜ t+τ (i) + (1 − γ )t+τ (i))
t+τ (1 − γ )τ −1 (γ 

τ =1

subject to the total demand it faces

X t (i) =

Pt (i)
Pt

1
 θ −1

Yt ,

˜ t+τ (i) = ( p̃t+τ (i) − Pt+τ St+τ )X (i, s t+τ ) is the profit attained when the
and where 
price is reset, while t+τ (i) = (π̄ τ p̃t (i) − Pt+τ St+τ )X t+τ (i) is the profit attained
when the price is maintained.  t+τ is an appropriate discount factor related to the

FABRICE COLLARD AND HARRIS DELLAS

:

719

way the household values future as opposed to current consumption. This leads to the
price setting equation

p̃t (i) =

1
θ

Et

∞



1

(1 − γ )π̄ θ −1

τ =0
∞


Et



τ
θ

(1 − γ )π̄ θ −1

2−θ

1−θ
t+τ Pt+τ
St+τ Yt+τ

τ

1

.

(10)

θ −1
t+τ Pt+τ
Yt+τ

τ =0

Since the price setting scheme is independent of any firm-specific characteristic, all
firms that reset their prices will choose the same price.
In each period, a fraction γ of contracts ends, so there are γ (1 − γ ) contracts
surviving from period t − 1, and therefore γ (1 − γ ) j from period t − j. Hence, from
(8), the aggregate intermediate price index is given by

Pt =

∞

j=0


γ (1 − γ ) j

p̃t− j
π̄ j

θ−1
θ 	 θ
 θ −1

.

(11)

1.4 The Monetary Authorities
We assume that monetary policy is conducted according to a standard HMT rule.
Namely,




Rt = ρ Rt−1 + (1 − ρ) R̄ + κπ E t (πt+1 − π ) + κ y yt − yt ,
where π t and yt are actual output and expected inflation, respectively, and π and y t are
the inflation and output targets, respectively. The output target is set equal to potential
output and the inflation target to the steady-state rate of inflation. Potential output
is defined to be the level of output that corresponds to the flexible price equilibrium
of our model. It is assumed that it is not observable and the monetary authorities
must learn about changes in it gradually. The learning process is described in the
Appendix. 4
There exists disagreement in the literature regarding the empirically relevant values
of κ π and κ y for the 1970s. Clarida, Gali, and Gertler claim that the pre-Volcker, HMT
monetary rule involved a policy response to inflation that was too weak. Namely, that
κ π < 1 which led to real indeterminacies and excessive inflation. They estimate the
triplet {ρ, κ π , κ y } = {0.75, 0.8, 0.4}. Orphanides disputes this claim. He argues that
the reaction to—expected—inflation was broadly similar in the pre and post-Volcker
period, but the reaction to output was stronger in the earlier period. In particular, using
real-time date, he estimates {ρ, κ π , κ y } = {0.75, 1.6, 0.6}.
We investigate the consequences of using alternative values for κ π and κ y in order
to shed some light on the role of policy preferences relative to that of the degree of
imperfect information for the behavior of inflation.
4. See Ehrmann and Smets (2003) for a discussion of optimal monetary policy in a related model.

720

:

MONEY, CREDIT AND BANKING

TABLE 1
CALIBRATION: BENCHMARK CASE
Preferences
Discount factor
Relative risk aversion
Parameter of CES in utility function
Weight of money in the utility function
CES weight in utility function

β
σ
η
ζ
ν

0.988
1.500
−1.560
0.065
0.344

α
ϕ
δ
θ
γ

0.281
1.000
0.025
0.850
0.250

ρa
σa
ρg
σg
g/y
μ

0.950
0.008
0.970
0.020
0.200
1.012

Technology
Capital elasticity of intermediate output
Capital adjustment costs parameter
Depreciation rate
Parameter of markup
Probability of price resetting
Shocks and policy parameters
Persistence of technology shock
Standard deviation of technology shock
Persistence of government spending shock
Volatility of government spending shock
Goverment share
Nominal growth

1.5 The Government
The government finances government expenditure on the domestic final good using
lump-sum taxes. The stationary component of government expenditures is assumed
to follow an exogenous stochastic process, whose properties will be defined later.

2. PARAMETERIZATION
The model is parameterized on U.S. quarterly data for the period 1960:1–1999:4.
The data are taken from the Federal Reserve Database. 5 The parameters are reported
in Table 1.
The discount factor,β, is set such that households discount the future at a 4% annual
rate, implying β equals 0.988. The instantaneous utility function takes the form
⎤
⎡
	1−σ



η  ην
1 ⎣
Mt
M
η
U Ct ,
, t =
1−ν
− 1⎦ ,
Ct + ζ t
t
Pt
1−σ
Pt
where ζ captures the preference for money holdings of the household. The coefficient,
σ , ruling risk aversion, is set equal to 1.5. ν is set such that the model generates a
5. URL: http://research.stlouisfed.org/fred/.

FABRICE COLLARD AND HARRIS DELLAS

:

721

total fraction of time devoted to market activities of 31%. η is borrowed from Chari,
Kehoe, and McGrattan (2000), who estimated it on postwar US data (−1.56). The
value of ζ , 0.0649, is selected such that the model mimics the average ratio of M1
money to nominal consumption expenditures.
γ , the probability of price resetting, is set in the benchmark case at 0.25, implying
that the average length of price contracts is about four quarters. The nominal growth
of the economy, μ, is set such that the average quarterly rate of inflation over the
period is π̄ = 1.2% per quarter.
The quarterly depreciation rate, δ, was set equal to 0.025. θ in the benchmark case
is set such that the level of markup in the steady state is 15%. α, the elasticity of the
production function to physical capital, is set such that the model reproduces the U.S.
labor share—defined as the ratio of labor compensation over GDP—over the sample
period (0.575).
The evolution of technology is assumed to contain two components. One capturing deterministic growth and the other stochastic growth. The stochastic one, at =
log (At /A) is assumed to follow a stationary AR(1) process of the form
at = ρa at−1 + εa,t
with |ρ a | < 1 and εa,t N (0, σa2 ). We set ρ a = 0.95 and 6 σ a = 0.008.
Alternative descriptions of the productivity process may be equally plausible. For
instance, productivity growth may have followed a deterministic trend that permanently shifted downward in the late 1960s to early 1970s. 7 In our model, this would
mean that the FED learns about the trend in productivity rather than about the current
level of the—temporary—shock to productivity. We are unsure about how our results
would be affected by using an alternative process, but, given the state of the art in this
area, we do not think that it is possible to identify the productivity process with any
degree of confidence.
The government spending shock. 8 is assumed to follow an AR(1) process
log(gt ) = ρg log(gt−1 ) + (1 − ρg ) log(ḡ) + εg,t
with |ρ g | < 1 and εg,t ∼ N (0, σg2 ). The persistence parameter is set to, ρ g , of 0.97
and the standard deviation of innovations is σ g = 0.02. The government spending to
output ratio is set to 0.20.

6. There is a non-negligible change in the volatility of the Solow residual between the pre- and postVolcker period. That up to 1979:4 is 0.0084 while that after 1980:1 is 0.0062. For the evaluation of the
model it is the former period that is relevant. Note that for the government spending shock the difference
between the two periods is negligible.
7. For instance, this is the assumption made by Bullard and Eusepi (2003). Nonetheless, there is very
little agreement regarding the type of change in the productivity process that took place around 1970. Other
differences between our model and that of Bullard and Eusepi are to be found the learning mechanism and
the interest policy rule employed.
8. The logarithm of the government expenditure series is first detrended using a linear trend.

722

:

MONEY, CREDIT AND BANKING

An important feature of our analysis is that there is imperfect information about the
true state of the economy. We assume that the agents observe their own, individual
specific variables (consumption, technology shock, capital stock, and so on) but cannot
always aggregate this information. As a result they do not fully know the true current
aggregate state of the economy and can only gradually learn about it using the Kalman
filter, based on a set of perfectly and imperfectly observable aggregate variables
(signals). Similarly, we assume that the monetary authorities also have imperfect
information about the state of the economy. In particular, some of the aggregate
variables may be perfectly observed, some may not be observed at all and some may
be observed with error by the agents. In particular, for the mismeasured variable xt ,
we have
xt = xtT + ξt ,
where xtT denotes the value of the variable under perfect information and ξ t is a noisy
process that satisfies E(ξ t ) = 0 for all t; E(ξ t ε a,t ) = E(ξ t ε g,t ) = 0; and

σξ2 if t = k
E(ξt ξk ) =
0
otherwise.
Obviously, for the informational considerations emphasized in this paper to be
operative, it is essential that the specification of information does not allow agents
to immediately infer the true state of the economy based on the available signals.
Otherwise, the signal extraction problem disappears, the agents always know the
state of the economy and the model collapses to that under perfect information. There
are various ways to ensure a meaningful signal extraction problem. One is to have
many variables that are subject to noise. An equivalent but perhaps less transparent
way is to reduce the model through substitution to a small number of equations and
variables and then impose noise on a smaller number of variables. We have opted for
the first approach. The signals we give to the monetary authorities are on potential
and also actual ouput. 9
In order to facilitate the interpretation of σ ξ we set its value in relation to the volatility of the technology shock. More precisely, we define ς as ς = σ ξ /σ a . Different
values were assigned to ς in order to gauge the effects of imperfect information in
the model.
3. THE RESULTS
The model is first log-linearized around the deterministic steady state and then
solved.
We start by using the standard specification for the HMT rule, namely, ρ = 0.75,
κ π = 1.5, and κ y = 0.5 (hereafter we denote  = {ρ r , κ π , κ y }) and vary the degree
9. Allowing for a signal on inflation does not change the results.

FABRICE COLLARD AND HARRIS DELLAS

:

723

of uncertainty—the quality of the signal—about potential output. 10 The objective
of this exercise is to determine (i) whether a policy reaction function of the type
commonly attributed to the FED during the 1980s and 1990s is consistent with high
and persistent inflation of the type observed in the 1970s, and (ii) the role played by
imperfect information. This exercise may then prove useful for determining whether
the great inflation can be attributed mostly to bad luck and incomplete information
(as Orphanides 2003, 2004 has argued). Or to insufficiently aggressive reaction to
inflation developments—a low κ π , as emphasized by Clarida, Gertler, and Gali (2000).
Or to an inherent inflation bias, as emphasized by Ireland (1999).
We report two sets of statistics. The volatility of H-P filtered actual output, annualized inflation, and investment. And the impulse response functions (IRF) of actual
output and inflation following a negative technology shock for the perfect information
model (Perf. Info.), the imperfect information model with ς = 1 (Imp. Info. (I)) and
ς = 8 (Imp. Info. (II)). The IRF for the inflation rate is annualized and expressed in
percentage points. The actual rate of inflation following a shock is simply found by
adding the response reported in the IRF to the steady state-value (π̄ = 4.8%).
There exists considerable uncertainty about the (type and) size of the shock that
triggered the productivity slowdown of the 1970s. We do not take a position on
this. We proceed by selecting a value for the supply shock that can generate a large
and persistent increase in the inflation rate under at least one of the informational
assumptions considered. By large, we mean an increase in the inflation rate of the
order of 5–7 percentage points, implying that the maximum rate of inflation obtained
during that period is about 10%–12%. We then feed a series of shocks that include
this value for the first quarter of 1973 into our model and generate the other statistics
described above.
Figure 1 reports the IRFs in the case of a standard HMT rule. The model can produce a large and persistent increase in the inflation rate if two conditions are met: The
shock is very large (of the order of 33%) and the degree of imperfect information is
very high (say, ς = 8). Moreover, Table 3 indicates that the model can generate a
realistic degree of macroeconomic volatility in the case of a high degree of imperfect
information. For instance, the volatility of output, investment and inflation in the case
γ = 0.25 (4 quarters contracts) and ς = 8 (Imp. Info (II)) are 1.820%, 6.736%, and
0.619%, respectively, to be compared to 1.639%, 7.271%, and 0.778% in the data. The
model fails, though, in its prediction of the maximal effect on output following such a
shock. In particular, the maximal predicted effect is −19.812% which seems implausibly high (Table 2). On the other hand, the performance of the model under perfect
information is bad. The increase in inflation is quite small, output and investment
volatility is too large and inflation volatility too low, and the maximal effects are even
higher.
Imperfect information is critical for the ability of the model to generate a persistent
increase in inflation as well as sufficient volatility following a persistent supply shock.

10. To be more precise, We vary the size of ς .

724

:

MONEY, CREDIT AND BANKING

FIG. 1. IRF to a −33% Technology Shock.
NOTE: Perf. Info, Imp. Info (I) and Imp. Info (II) correspond to ς = 0, 1, 8 respectively, where ς is the amount of noise.
{ρ, κ π , κ y = κ y ∗ } = {0.75, 1.50, 0.50}.

TABLE 2
EFFECT OF A −33% TECHNOLOGY SHOCK
Perf. info

Output
Inflation

Imp. info (I)

Imp. info (II)

Impact

Max

Impact

Max

Impact

Max

−45.074
0.335

−45.074
1.543

−29.977
2.597

−38.695
2.597

−3.163
6.569

−20.803
6.569

NOTE: Perf. Info, Imp. Info (I) and Imp. Info (II) correspond to ς = 0, 1, 8 respectively, where ς is the amount of noise. {ρ, κ π , κ y = κ y ∗ } =
{0.75, 1.50, 0.50}.

TABLE 3
STANDARD DEVIATIONS (−33% TECHNOLOGY SHOCK)

Data
Perf. info.
Imp. info. (I)
Imp. info. (II)

σy

σi

σπ

1.639
4.349
3.891
1.820

7.271
15.625
14.324
6.736

0.778
0.097
0.212
0.619

NOTE: The standard deviations are computed for HP–filtered series. y, i and π are output, investment and inflation respectively. Perf. Info,
Imp. Info (I) and Imp. Info (II) correspond to ς = 0, 1, 8 respectively, where ς is the amount of noise. {ρ, κ π , κ y = κ y ∗ } = {0.75, 1.50, 0.50}.

When the variance of the noise is large, much of the change in actual inflation is attributed to cyclical rather than “core” developments. This means that estimated future
inflation—and hence the inflation “gap”— is sticky; i.e., it does not move much with
the current shocks and actual inflation (see Figure 2). Imperfect information introduces a serially correlated error term in the Phillips curve, whose size and persistence
depend on the size of κ π and the speed of learning. As a result, the policy reaction
to a perceived small inflation gap proves too weak even if κ π is large, resulting in

FABRICE COLLARD AND HARRIS DELLAS

:

725

FIG. 2. Expected Versus Realized Inflation Rate (−33% Technology Shock).
NOTE: {ρ, κ π , κ y = κ y ∗ } = {0.75, 1.50, 0.50}.

FIG. 3. Ex Ante versus Ex Post Real Interest Rate (−33% Technology Shock).
NOTE: {ρ, κ π , κ y = κ y ∗ } = {0.75, 1.50, 0.50}.

countercyclical policy. The real interest rate is decreased significantly, see Figure 3,
fueling inflation while smoothing output out. As long as the inflation forecast error
is persistent (as this will be the case for a persistent shock and slow learning) the
increase in actual inflation will be persistent too. This requirement does not seem to
pose a problem for the model as the magnitude of the predicted gap between actual
and expected inflation seems to be in line with that observed in the 1970s.
The choice of the inflation variable that enters the policy rule plays an important
role. The argument above has suggested that the source of the persistence in inflation
is the stickiness of expected inflation. Were the FED to react to current or past actual
inflation relative to target then inflation would be contained more quickly. In this
case, however, the model would behave less satisfactorily. Inflation volatility would
be further away from that in the data, output volatility would be exaggerated and
the maximal effect on output would be even higher. Thus, excessive policy maker
optimism about the future inflation path plays an important role.

726

:

MONEY, CREDIT AND BANKING

The strength of the stabilization motive (the coefficient κ y ) does not play an important role in the analysis. We have repeated the analysis under κ π = 1.2 and
κ y = 0.7 with almost identical results. 11 This is a comforting finding because it
is difficult to justify differences in stabilization motives between the pre-and post1980 policy makers. Differences in luck and information are much less controversial.
The model does not perform as well with a lower κ π . In this case, it is difficult to
both match volatility and generate the appropriate inflation dynamics. If the model
matches volatility well then it exaggerates the increase in inflation.
Increasing the degree of price flexibility (say, from γ = 0.25 to γ = 1/3) does
not alter the basic picture but improves things somewhat. A smaller shock is now
required, inflation volatility moves closer to that in the data and the maximal effect
on output is reduced. At the same time, inflation persistence is somewhat reduced.
We have run a larger number of experiments involving this HMT rule and alternative
values of the other parameters of the model and found similar results. In particular,
we have experimented with: (i) A specification of the Phillips curve that contains a
lagged inflation term. Such a specification could be the result of the assumption that
the firms that are not given the opportunity to optimally reset their prices simply index
their price to lagged inflation. The resulting hybrid Phillips curve is generally thought
to have superior empirical properties relative to the purely forward one. (ii) Different
(higher) values for the coefficient of capital adjustment costs. (iii) A specification
of the policy rule under which the FED reacts to actual output rather than to the
output gap. (iv) Different values for some other parameters of the model. We have
also run experiments involving different parameterizations of the HMT policy rule. 12
In none of these cases, there was a noticeable reduction in the required volatility
of the technology shock (and in the case of the hybrid Phillips curve, the required
volatility even increased). For instance, with a high capital adjustment costs (ϕ =
20), the required shock decreases very little to −30%, but at the same time, the other
properties of the model deteriorate.
To summarize our main results: The NK model under the standard HMT policy
rule and imperfect information can generate plausible inflation dynamics and good
overall fit in the face of a misperceived, extraordinarily large productivity slowdown,
and expected inflation gap targeting. Nonetheless, in addition to the requirement on
the size of the shock, this specification has the weaknesses, that it implies a very
severe recession.
We now turn to specifications in which policy is conducted in a way that destabilizes
rather than constrains inflation (as suggested by Clarida, Gertler, and Gali 2000). We
have investigated the properties of the model under the policy rule parameterization
suggested by CGG, namely, ρ r = 0.75, κ π = 0.80, κ y = 0.40. Such a rule leads
to real indeterminacy. This specification can generate a large, persistent increase in

11. These results are reported in the technical appendix to this paper, which can be downloaded from
http://www.vwi.unibe.ch/amakro/res.
12. All these results are available in the technical appendix to this paper, available on http://www.
vwi.unibe.ch/amakro/res.

FABRICE COLLARD AND HARRIS DELLAS

:

727

FIG. 4. IRF to a −12% Technology Shock.
NOTE: {ρ, κ π , κ y = κ y ∗ } = {0.75, 0.80, 0.40}.

TABLE 4
EFFECTS OF A −12% TECHNOLOGY SHOCK

Output
Inflation

Impact

Max.

−1.773
5.000

−12.755
5.000

NOTE: {ρ, κ π , κ y = κ y ∗ } = {0.75, 0.80, 0.40}.

TABLE 5
STANDARD DEVIATIONS (−12% TECHNOLOGY SHOCK)
σs

σy

Data

1.639

0
σa
0.0400(a)
0.0294(b)
0.1294(c)

1.702
1.727
2.272
2.030
5.065

σi

7.271
q = 0.25, −12% Shock
5.545
5.689
8.463
7.278
21.029

σπ

0.778
0.529
0.542
0.777
0.676
1.861

NOTES: The standard deviations are computed for HP-filtered series. y, i, and π are output, investment and inflation, respectively. (a), (b), and
(c) match σ π , σ i , and σ R . {ρ, κ π , κ y = κ y ∗ } = {0.75, 0.80, 0.40}.

inflation (see Figure 4), but the associated response of output is implausible and
macroeconomic volatility is too low (Tables 4 and 5).
An important feature of this specification is that real indeterminacy introduces
an additional source of uncertainty related to a sunspot shock that affects beliefs.
We assume that the sunspot shock is purely extrinsic and is therefore not correlated
with any fundamental shock. Since we have no information that would allow us to
calibrate this shock we have explored several cases. In the first one, the volatility of

728

:

MONEY, CREDIT AND BANKING

FIG. 5. IRF to a −8% Technology Shock.
NOTE: {ρ, κ π , κ y = κ y ∗ } = {0.75, 1.20, 0.80}.

TABLE 6
EFFECTS OF A −8% TECHNOLOGY SHOCK,  = {0.75, 1.20, 0.80}

Output
Inflation

Impact

Max.

−1.718
5.020

−9.972
5.020

NOTE: {ρ, κ π , κ y = κ y ∗ } = {0.75, 1.20, 0.80}.

the sunspot shock is set to 0. In this case, the model overestimates output volatility,
but significantly underestimates that of investment, consumption, and inflation. This
is also the case when the volatility is set at the same level as that of the technology
shock. When the sunspot shock is calibrated in order for the model to match inflation
volatility, the implied standard deviation of output is widely overestimated (by almost
40%). The same obtains when the sunspot is calibrated to match investment volatility,
and this is highly magnified when the sunspot is used to mimic the volatility of the
nominal interest rate. 13
Nonetheless, we have encountered more successful policy specifications within the
range of indeterminate equilibria. Figure 5 and Tables 6 and 7 correspond to such
a case with ρ r = 0.75, κ π = 1.20, κ y = 0.80. As can be seen, this specification
performs fairly well. The model has little difficulty producing high and persistent
inflation and can account for volatility fairly well (but it underestimates investment
volatility). If it has an Achilles heel, it is to be found in its excessive reaction of
output (Figure 5), a weakness that it shares with the imperfect information version
under the standard HMT rule. Hence, the main advantage of this specification may
be that it works even with a much smaller shock relative to the case of determinacy
13. We could not set the sunspot volatility so as to match consumption volatility as it is already
overestimated when the standard deviation of the sunspot is set to 0.

FABRICE COLLARD AND HARRIS DELLAS

:

729

TABLE 7
STANDARD DEVIATIONS,  = {0.75, 1.20, 0.80}
σs

Data
0
σa
0.006(a)
0.035(b)
0.016(c)
0.058(d)

σy

σi

σπ

1.639
1.625
1.650
1.639
2.072
1.724
2.681

7.271
5.274
5.394
5.340
7.271
5.736
9.827

0.778
0.689
0.714
0.704
1.042
0.778
1.461

NOTES: The standard deviations are computed for HP-filtered series. y, i, and π are output, investment, and inflation, respectively. (a), (b),
(c), and (d) match σ y , σ i , σ π , and σ R . {ρ, κ π , κ y = κ y ∗ } = {0.75, 1.20, 0.80}.

(but still the shock is quite large in absolute terms). It is also worth mentioning
that other specifications (higher adjustment costs, a hybrid Phillips curve) do not
improve much on this front (these results appear in the technical appendix available
at http://www.vwi.unibe.ch/amakro/res).
How can we explain the similarity in the results under the two specifications of the
policy rule? The reaction of the nominal interest rates to inflation is the product of the
inflation reaction coefficient and the estimated inflation “gap.” High and persistent
inflation can occur following a productivity slowdown either because the reaction
coefficient is low (the CGG scenario of bad policy) or because the estimated inflation
gap to which policy is reacting is low (the Orphanides scenario of imperfect information). This reasoning indicates that there may be a serious difficulty in identifying
the policy rule. The difference in the results of CGG and Orphanides who rely on
different information assumptions (actual vs. real-time data) can be explained using
this argument.
Before concluding, let us point out that there is a widespread belief that the great
inflation did not actually start in the early 1970s but rather in the mid-1960s. In our
model, a series of unperceived negative supply shocks, culminating with an oil shock
in 1973—that was misperceived as temporary—can reproduce the upward trend as
well as the spike in the inflation series. 14
4. CONCLUSIONS
Inflation in the United States reached high levels during the 1970s, to a large
extent due to what proved to be excessively loose monetary policy. There exist two
conflicting views concerning the conduct of policy at that time. One sees it as reflecting
opportunistic (discretionary) behavior on the part of the FED (Barro and Gordon
1983). According to this view, the problem of inflation arises from poorly designed
14. There is considerable evidence, based, for instance, on the behavior of the current account, that the
increase in the oil price in 1973 was perceived as temporary.

730

:

MONEY, CREDIT AND BANKING

institutions, and the only way to prevent inflationary episodes in the future is by
creating institutions that provide the “right” incentives to the policy makers.
The other view attributes looseness to inadvertent policy mistakes committed by a
central bank that follows a rule. Such mistakes can arise even when the central bank
is sufficiently averse to inflation, due to imperfect information about the true state
of the economy (Orphanides 2003). Or when the bank does not fully understand the
properties of the rule it uses (Clarida, Gali, and Gertler 2000). The recommended
solution in these cases is to improve the technical aspects of policy making, that is,
to adopt better rules, allow for imperfect information and so on.
Our analysis has established that policy opportunism is not necessary for obtaining
persistently bad inflation outcomes as long as one is prepared to accept the occurrence of an extraordinarily large supply shock. Under such circumstances, these two
rule-based explanations produce empirically compelling predictions. Nevertheless,
the information contained in the data does not suffice to conclusively discriminate
between. Additional races are needed. Although Lubik and Schforheide (2004) argue that the data support a policy specification with indeterminacy over one with
determinacy (for the 1970s) their model does not include the key elements emphasized by Orphanides. We are currently investigating this issue using the Lubik and
Schforheide methodology but also incorporating learning on the part of the policy
makers. Whether this approach will break the observational equivalence between the
competing theories remains an open issue.

LITERATURE CITED
Barro, Robert, and David Gordon. (1983) “Rules, Discretion and Reputation in a Model of
Monetary Policy.” Journal of Monetary Economics, 12:1, 101–21.
Bils, Mark, and Peter Klenow. (2002) “Some Evidence on the Importance of Sticky Prices.”
Journal of Political Economy, 112, 947–85.
Bullard, James, and Stefano Eusepi. (2003) “Did the Great Inflation Occur Despite Policymaker
Commitment to a Taylor Rule.” Federal Reserve Bank of Atlanta, October, WP 2003-20.
Canova, Fabio. (2005) “Monetary Policy and the Evolution of US Economy: 1948-2002.”
Mimeo.
Chari, V., Patrick Kehoe, and Ellen McGrattan. (2000) “Can Sticky Price Models Generate
Volatile and Persistent Real Exchange Rate Changes?” NBER Working Paper No. 7869.
Christiano, Laurence, and Christopher Gust. (1999) “The Great Inflation of the 1970s.” Mimeo.
Clarida, Richard, Jordi Gali, and Mark Gertler. (2000) “Monetary Policy Rules and Macroeconomic Stability: Evidence and Some Theory.” Quarterly Journal of Economics, 115:1,
147–80.
Cukierman, Alex, and Francesco Lippi. (2005) “Endogenous Monetary Policy with Unobserved
Potential Output.” Journal of Economics Dynamics and Control, 29:11, 1951–83.
DeLong, Bradford. (1997) “America’s Peacetime Inflation: The 1970s.” In Reducing Inflation:
Motivation and Strategy, edited by C. Romer and D. Romer, pp. 247–76. Chicago: University
of Chicago Press.

FABRICE COLLARD AND HARRIS DELLAS

:

731

Ehrmann, Michael, and Frank Smets. (2003) “Uncertain Potential Output: Implications for
Monetary Policy.” Journal of Economic Dynamics and Control, 27, 1611–38.
Erceg, Christopher, and Andrew Levin. (2003) “Imperfect Credibility and Inflation Persistence.” Journal of Monetary Economics, 50:4, 915–44.
Ireland, Peter. (1999) “Does the Time-Consistency Problem Explain the Behavior of Inflation
in the United States?” Journal of Monetary Economics, 44:2, 279–91.
Lansing, Kevin J. (2001) “Learning about a Shift in Trend Output: Implications for Monetary
Policy and Inflation.” Unpublished manuscript. FRB San Francisco.
Lubik, Thomas, and Schrofheide, Franck. (2004) “Testing for Indeterminacy: An Application
to U.S. Monetary Policy.” American Economic Review, 94:1, 190–217.
Nelson, Edward, and Kalin Nicolov. (2002) “Monetary Policy and Stagflation in the UK”
CEPR Discussion Paper No. 3458, July.
Orphanides, Athanasios. (2003) “The Quest for Prosperity without Inflation.” Journal of Monetary Economics, 50:3, 633–63.
Orphanides, Athanasios. (2004) “Monetary Policy Rules, Macroeconomic Stability and Inflation: A View from the Trenches.” Journal of Money, Credit and Banking, 36, 151–77.
Orphanides, Athanasios, and John C. Williams. (2005) “The Decline of Activist Stabilization
Policy: Natural Rate Misperceptions, Learning and Expectations.” Journal of Economic
Dynamics and Control, 29, 1927–50.
Sargent, Thomas J. (1999) The Conquest of American Inflation. Princeton, NJ: Princeton University Press.
Svensson, Lars, and Michael Woodford. (2003) “Indicator Variables for Optimal Policy.” Journal of Monetary Economics, 50:3, 691–720.

